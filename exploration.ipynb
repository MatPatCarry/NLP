{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d5afc8-8486-45f2-b2b1-5b1d7aa7135e",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a18fab-8c9c-47b5-815e-5aea6f853959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konrad-kielczynski/miniconda3/envs/golem-ner/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm, trange\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ddef26-399f-4798-a1e1-a10b59896077",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0486a4d3-6094-4699-a2df-13c841910e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALNUM_CHARSET = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "POLISH_CHARS = set('ąćęłńóśźżĄĆĘŁŃÓŚŹŻ')\n",
    "ALNUM_CHARSET.update(POLISH_CHARS)\n",
    "\n",
    "def convert_to_tokens(indices, tokenizer, extended=False, extra_values_pos=None, strip=True):\n",
    "    if extended:\n",
    "        res = [tokenizer.convert_ids_to_tokens([idx])[0] if idx < len(tokenizer) else \n",
    "               (f\"[pos{idx-len(tokenizer)}]\" if idx < extra_values_pos else f\"[val{idx-extra_values_pos}]\") \n",
    "               for idx in indices]\n",
    "    else:\n",
    "        res = tokenizer.convert_ids_to_tokens(indices)\n",
    "    if strip:\n",
    "        res = list(map(lambda x: x[1:] if x[0] == 'Ġ' else \"#\" + x, res))\n",
    "    return res\n",
    "\n",
    "\n",
    "def top_tokens(v, k=100, tokenizer=None, only_alnum=False, only_ascii=True, with_values=False, \n",
    "               exclude_brackets=False, extended=True, extra_values=None, only_from_list=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = my_tokenizer\n",
    "    v = deepcopy(v)\n",
    "    ignored_indices = []\n",
    "    if only_ascii:\n",
    "        ignored_indices.extend([key for val, key in tokenizer.vocab.items() if not val.strip('Ġ▁').isascii()])\n",
    "    if only_alnum: \n",
    "        ignored_indices.extend([key for val, key in tokenizer.vocab.items() if not (set(val.strip('Ġ▁[] ')) <= ALNUM_CHARSET)])\n",
    "    if only_from_list:\n",
    "        ignored_indices.extend([key for val, key in tokenizer.vocab.items() if val.strip('Ġ▁ ').lower() not in only_from_list])\n",
    "    if exclude_brackets:\n",
    "        ignored_indices = set(ignored_indices).intersection(\n",
    "            {key for val, key in tokenizer.vocab.items() if not (val.isascii() and val.isalnum())})\n",
    "        ignored_indices = list(ignored_indices)\n",
    "        \n",
    "    ignored_indices = list(set(ignored_indices))\n",
    "    v[ignored_indices] = -np.inf\n",
    "    extra_values_pos = len(v)\n",
    "    if extra_values is not None:\n",
    "        v = torch.cat([v, extra_values])\n",
    "    values, indices = torch.topk(v, k=k)\n",
    "    res = convert_to_tokens(indices, tokenizer, extended=extended, extra_values_pos=extra_values_pos)\n",
    "    if with_values:\n",
    "        res = list(zip(res, values.cpu().numpy()))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e03a094-4602-47bf-89ea-0656fb4c1d5d",
   "metadata": {},
   "source": [
    "## Extract Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8b3e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"sdadas/polish-gpt2-medium\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "tokenizer = my_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95361220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 1024)\n",
       "    (wpe): Embedding(2048, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): FastGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6e82d2c-92a5-4892-afc3-b40439d6b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "tokenizer = my_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "emb = model.get_output_embeddings().weight.data.T.detach()\n",
    "\n",
    "num_layers = model.config.n_layer\n",
    "num_heads = model.config.n_head\n",
    "hidden_dim = model.config.n_embd\n",
    "head_size = hidden_dim // num_heads\n",
    "\n",
    "K = torch.cat([model.get_parameter(f\"transformer.h.{j}.mlp.c_fc.weight\").T\n",
    "                           for j in range(num_layers)]).detach()\n",
    "V = torch.cat([model.get_parameter(f\"transformer.h.{j}.mlp.c_proj.weight\")\n",
    "                           for j in range(num_layers)]).detach()\n",
    "\n",
    "W_Q, W_K, W_V = torch.cat([model.get_parameter(f\"transformer.h.{j}.attn.c_attn.weight\") \n",
    "                           for j in range(num_layers)]).detach().chunk(3, dim=-1)\n",
    "W_O = torch.cat([model.get_parameter(f\"transformer.h.{j}.attn.c_proj.weight\") \n",
    "                           for j in range(num_layers)]).detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "104fd920-f247-4827-be5a-c958dfadbe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_heads = K.reshape(num_layers, -1, hidden_dim)\n",
    "V_heads = V.reshape(num_layers, -1, hidden_dim)\n",
    "d_int = K_heads.shape[1]\n",
    "\n",
    "W_Q_heads = W_Q.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n",
    "W_K_heads = W_K.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n",
    "W_V_heads = W_V.reshape(num_layers, hidden_dim, num_heads, head_size).permute(0, 2, 1, 3)\n",
    "W_O_heads = W_O.reshape(num_layers, num_heads, head_size, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cb36e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 16, 1024, 64])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Q_heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cec5b99d-9e6c-413f-a43d-02fd39485640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51200, 1024])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_inv = emb.T\n",
    "emb_inv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b9e38-8ff4-41f6-8c47-8ae23f4293e6",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b3452-6449-4397-bda7-3209c573c53a",
   "metadata": {},
   "source": [
    "#### Alternative I: No Token List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fbe8ce6-abf5-427b-b374-a43f88b06097",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada3bfc9-3d15-43ae-a21c-da0747b16ba4",
   "metadata": {},
   "source": [
    "#### Alternative II: Can Load Token List from IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bb14a77-5ac2-449b-b771-97be619cb79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fe48f8f-eb62-4bee-a45c-27ff12220825",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = load_dataset('clarin-knext/wsd_polish_datasets')['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae8eb81a-8525-40ef-8376-a902980c18d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens_num = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea6fd97b-1dbf-4e61-bcdb-f4f2d2c23ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7848/7848 [00:07<00:00, 1079.15it/s]\n"
     ]
    }
   ],
   "source": [
    "if max_tokens_num is None:\n",
    "    tokens_list = set()\n",
    "    for txt in tqdm(imdb):\n",
    "        tokens_list = tokens_list.union(set(tokenizer.tokenize(txt)))\n",
    "else:\n",
    "    tokens_list = Counter()\n",
    "    for txt in tqdm(imdb):\n",
    "        tokens_list.update(set(tokenizer.tokenize(txt)))\n",
    "    tokens_list = map(lambda x: x[0], tokens_list.most_common(max_tokens_num))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f2944cdd-13d9-41ea-a2d8-a643ab49b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = set([*map(lambda x: x.strip('Ġ▁').lower(), tokens_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71b325-da96-47b2-99a8-6b909668b642",
   "metadata": {},
   "source": [
    "### FF Keys & Values\n",
    "\n",
    "      Emb = (1024, 50257)\n",
    "\n",
    "       (mlp): GPT2MLP(\n",
    "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
    "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
    "          ...\n",
    "       )\n",
    "\n",
    "- projection of c_fc(key) -> c_key.T @ Emb\n",
    "- projection of c_proj(value) -> c_value @ Emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "87c2e56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 4096, 1024])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eda18b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K_heads[23, 907].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3e7bc3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 907\n",
      "K          V              -K          -V\n",
      "---------  -------------  ----------  ---------\n",
      "przycu     kody           dotychczas  #bot\n",
      "zalog      #gory          rodzi       #lot\n",
      "wylegi     #ei            do          #dzista\n",
      "#walifik   #zmy           przebie     #remont\n",
      "przesp     Apo            gatunku     #lee\n",
      "pochowany  apokali        przez       #wan\n",
      "#ppe       #128           #ja         #ette\n",
      "wep        ludy           rodzin      #up\n",
      "sfinans    #ords          zrazu       #bul\n",
      "#iss       Cezary         lokalnie    #puszczam\n",
      "#CS        archa          porywa      spu\n",
      "#gny       przy           two         zamyka\n",
      "#zwol      Homo           jeszcze     odstawi\n",
      "#-).       litera         #jaw        #mont\n",
      "lock       #pka           na          #beki\n",
      "skonfisk   Benedykt       wymaga      #laks\n",
      "#post      narodem        ty          tap\n",
      "postoju    polityki       Drze        poby\n",
      "erek       symbolu        pod         posto\n",
      "#ionu      #lachet        Nie         #wana\n",
      "#ppo       publicznego    rzuca       #dzana\n",
      "unierucho  #rzami         ludzkim     #load\n",
      "#ott       anie           uczucia     #owol\n",
      "zbombar    uniwersalny    rze         zatk\n",
      "#FF        artystycznego  tak         opuszcza\n",
      "#prem      ustawowego     da          znajd\n",
      "#klaski    werb           ludzkie     #dzany\n",
      "#gs        wzroku         natury      #elli\n",
      "zatk       #chnienia      #obie       #bek\n",
      "#hony      #osc           pewnego     zrealiz\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def display_top_tokens(i1=23, i2=907):\n",
    "    # i1, i2 = np.random.randint(num_layers), np.random.randint(d_int)\n",
    "\n",
    "    print(i1, i2)\n",
    "    print(tabulate([*zip(\n",
    "        top_tokens((K_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list),\n",
    "        top_tokens((V_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list),\n",
    "        top_tokens((-K_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list),\n",
    "        top_tokens((-V_heads[i1, i2]) @ emb, k=30, only_from_list=tokens_list),\n",
    "    )], headers=['K', 'V', '-K', '-V']))\n",
    "\n",
    "# Call the function\n",
    "display_top_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca722a1",
   "metadata": {},
   "source": [
    "# służby porządkowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8ae2c48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 345\n",
      "K             V           -K      -V\n",
      "------------  ----------  ------  ---------\n",
      "ubraniu       kondu       #./     #berga\n",
      "#czkom        #zyjny      #wiek   #CIA\n",
      "przebra       ubrania     #An     #dge\n",
      "ubiera        ubranie     #yn     #bul\n",
      "#jazdy        munduru     #obie   #cznikiem\n",
      "zakwater      mundury     #x      #dina\n",
      "ubrania       spoczynku   #Bud    Steve\n",
      "mundury       bieliz      ()      #CZY\n",
      "ciuchy        #rowo       #Ener   #tp\n",
      "wynajm        #cyjny      #wszy   #bergu\n",
      "masek         elegancki   #ws     Chinami\n",
      "skontak       #rysty      #zn     Lewis\n",
      "ubranie       przepu      Jun     #cym\n",
      "mieszkalnego  #lij        #Nu     #sonem\n",
      "telef         nocleg      #stor   Jedwab\n",
      "#ownikami     #anepi      Wen     #soci\n",
      "#jazdem       schlu       #poty   #CI\n",
      "pozory        fiska       #Gen    stad\n",
      "spak          oznaczenia  #()     #ith\n",
      "schroni       medyczna    #orga   Gary\n",
      "zapak         biletu      #chia   Warren\n",
      "#jazdu        mundur      #noza   #czo\n",
      "ubraniach     aresztu     #owski  Zach\n",
      "adiutan       #tarny      lewa    #berg\n",
      "telefonem     pretek      Zob     Szan\n",
      "desant        socjal      kad     dzban\n",
      "przesp        medyczne    #He     #Cie\n",
      "lusterku      przewie     #skrzy  #naniu\n",
      "dojazd        #zyjnych    #wie    #czaniu\n",
      "zalog         medyczny    #pot    Bet\n"
     ]
    }
   ],
   "source": [
    "display_top_tokens(12, 345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf1cba",
   "metadata": {},
   "source": [
    "# Instytucja, miasta, budulce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "915a5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K            V           -K             -V\n",
      "-----------  ----------  -------------  ----------\n",
      "#Ubra        #Drugi      #ord           kamy\n",
      "#Proto       #ros        #nem           kamieniu\n",
      "musku        #nalny      #dal           marek\n",
      "aminokwa     #owz        #siak          kamieniem\n",
      "Sztuk        Poni        #co            kamieni\n",
      "ubra         dziki       Rydzy          #latory\n",
      "#Produkty    #ama        #ens           #kien\n",
      "horm         Arnold      #ve            parasol\n",
      "#mato        Boj         #anna          kamienic\n",
      "#nospraw     Tob         Jawor          #kowca\n",
      "wychowan     #nu         gminie         telewizo\n",
      "sztan        #Sie        autostrady     walut\n",
      "elasty       Wro         autostra       ce\n",
      "#jet         #gust       Orange         #zione\n",
      "Mecha        #ness       #dale          wideo\n",
      "parami       #nad        tunelu         scen\n",
      "genetycznie  #alizm      archidiecezji  #eu\n",
      "#ship        #roe        parafii        #html\n",
      "Mocy         duchem      dokument       kamieniami\n",
      "dynamiki     #amy        #illa          kolumna\n",
      "#Mecha       Sand        #ne            #cyz\n",
      "pokarmowego  #Zale       #raz           kamienia\n",
      "#Nau         Odrodzenia  dojazd         #ugu\n",
      "kamize       1922        #ridge         #CT\n",
      "kok          #niesie     #naj           mili\n",
      "osobist      Kur         puszcza        fil\n",
      "#Kom         #owniczy    drogi          para\n",
      "Fizycznej    #nizacji    #is            #tv\n",
      "#nokwa       #owicze     diecezji       #tnika\n",
      "spado        #gas        Wester         ha\n"
     ]
    }
   ],
   "source": [
    "display_top_tokens(20, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8534204",
   "metadata": {},
   "source": [
    "# imiona medycyna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a1cf2848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 3424\n",
      "K               V           -K        -V\n",
      "--------------  ----------  --------  ----------\n",
      "#iry            #owaty      #kara     #loka\n",
      "dech            #syw        #yta      pogro\n",
      "#owz            #sista      poznamy   #lubie\n",
      "#rk             #Wzrost     #zna      #ranki\n",
      "#ott            #RL         #wizja    #ranka\n",
      "#ard            #Kru        Kamila    #komo\n",
      "parlamentarnej  #beta       #zera     pustymi\n",
      "czynnej         #karzem     #dun      trium\n",
      "#ine            #Lep        #nnik     #ryn\n",
      "szklanych       #znaczne    #lada     #nity\n",
      "mastur          #stem       #dem      #ion\n",
      "Lake            #Inten      #chodem   #ionu\n",
      "Fox             #Demon      #wid      #roczy\n",
      "#ross           #start      #blin     #nium\n",
      "bat             #Major      #styn     pia\n",
      "palcami         #Mateusz    #lad      Ludowego\n",
      "hazard          #Maksym     #cht      han\n",
      "medytacji       #znania     #Mateusz  #ATO\n",
      "baterii         #Bor        #gada     #obo\n",
      "antykoncep      zbli        Bartosz   zamykam\n",
      "#szczo          piersiowej  #pola     Toruniu\n",
      "#arki           #Pul        Boru      Rie\n",
      "Long            #sen        #manuel   #onie\n",
      "#anse           #Rat        #ska      #ink\n",
      "Ludowego        gazy        #chno     #ison\n",
      "gastro          #Alex       #mun      pusto\n",
      "chodzenie       #Zna        #znania   Torunia\n",
      "pasji           #Sprzeciw   ucho      zapomnieli\n",
      "#woo            #eth        #wiano    boje\n",
      "rozrywki        #Harry      #tusa     #ille\n"
     ]
    }
   ],
   "source": [
    "i1, i2 = np.random.randint(num_layers), np.random.randint(d_int)\n",
    "\n",
    "display_top_tokens(i1, i2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21fbd66",
   "metadata": {},
   "source": [
    "### Attention Weights Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ca7323f0-7cf9-4020-bbbd-191de7fe25b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_topk(mat, min_k=500, max_k=250_000, th0=10, max_iters=10, verbose=False):\n",
    "    _get_actual_k = lambda th, th_max: torch.nonzero((mat > th) & (mat < th_max)).shape[0]\n",
    "    th_max = np.inf\n",
    "    left, right = 0, th0 \n",
    "    while True:\n",
    "        actual_k = _get_actual_k(right, th_max)\n",
    "        if verbose:\n",
    "            print(f\"one more iteration. {actual_k}\")\n",
    "        if actual_k <= max_k:\n",
    "            break\n",
    "        left, right = right, right * 2\n",
    "    if min_k <= actual_k <= max_k:\n",
    "        th = right\n",
    "    else:\n",
    "        for _ in range(max_iters):\n",
    "            mid = (left + right) / 2\n",
    "            actual_k = _get_actual_k(mid, th_max)\n",
    "            if verbose:\n",
    "                print(f\"one more iteration. {actual_k}\")\n",
    "            if min_k <= actual_k <= max_k:\n",
    "                break\n",
    "            if actual_k > max_k:\n",
    "                left = mid\n",
    "            else:\n",
    "                right = mid\n",
    "        th = mid\n",
    "    return torch.nonzero((mat > th) & (mat < th_max)).tolist()\n",
    "\n",
    "def get_top_entries(tmp, all_high_pos, only_ascii=False, only_alnum=False, exclude_same=False, exclude_fuzzy=False, tokens_list=None):\n",
    "    remaining_pos = all_high_pos\n",
    "    if only_ascii:\n",
    "        remaining_pos = [*filter(\n",
    "            lambda x: (tokenizer.decode(x[0]).strip('Ġ▁').isascii() and tokenizer.decode(x[1]).strip('Ġ▁').isascii()), \n",
    "            remaining_pos)]\n",
    "    if only_alnum:\n",
    "        remaining_pos = [*filter(\n",
    "            lambda x: (tokenizer.decode(x[0]).strip('Ġ▁ ').isalnum() and tokenizer.decode(x[1]).strip('Ġ▁ ').isalnum()), \n",
    "            remaining_pos)]\n",
    "    if exclude_same:\n",
    "        remaining_pos = [*filter(\n",
    "            lambda x: tokenizer.decode(x[0]).lower().strip() != tokenizer.decode(x[1]).lower().strip(), \n",
    "            remaining_pos)]\n",
    "    if exclude_fuzzy:\n",
    "        remaining_pos = [*filter(\n",
    "            lambda x: not _fuzzy_eq(tokenizer.decode(x[0]).lower().strip(), tokenizer.decode(x[1]).lower().strip()), \n",
    "            remaining_pos)]\n",
    "    if tokens_list:\n",
    "        remaining_pos = [*filter(\n",
    "            lambda x: ((tokenizer.decode(x[0]).strip('Ġ▁').lower().strip() in tokens_list) and \n",
    "                       (tokenizer.decode(x[1]).strip('Ġ▁').lower().strip() in tokens_list)), \n",
    "            remaining_pos)]\n",
    "\n",
    "    pos_val = tmp[[*zip(*remaining_pos)]]\n",
    "    good_cells = [*map(lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])), remaining_pos)]\n",
    "    good_tokens = list(map(lambda x: Counter(x).most_common(), zip(*good_cells)))\n",
    "    remaining_pos_best = np.array(remaining_pos)[torch.argsort(pos_val if reverse_list else -pos_val)[:50]]\n",
    "    good_cells_best = [*map(lambda x: (tokenizer.decode(x[0]), tokenizer.decode(x[1])), remaining_pos_best)]\n",
    "    # good_cells[:100]\n",
    "    # list(zip(good_tokens[0], good_tokens[1]))\n",
    "    return good_cells_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd4e4a-a69b-4d58-a371-426b9073ff81",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### $W_{VO}$ Interpretation\n",
    "\n",
    "different heads capture different types of relations between pairs of vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79156807-54af-4fe8-b170-29dc0856d686",
   "metadata": {},
   "source": [
    "Choose **layer** and **head** here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f0d00edd-fdca-40d0-807f-e4bf3d7611e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 9)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = np.random.randint(num_layers), np.random.randint(num_heads)\n",
    "i1, i2 = 23, 9\n",
    "i1, i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fa70452c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 16, 1024, 64]), torch.Size([24, 16, 64, 1024]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_V_heads.shape, W_O_heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2743e277-74f7-41a6-a479-5bd65840d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_V_tmp, W_O_tmp = W_V_heads[i1, i2, :], W_O_heads[i1, i2]\n",
    "tmp = (emb_inv @ (W_V_tmp @ W_O_tmp) @ emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0ede26ee-3b82-4446-8b6d-2300e7bdcc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 87\n",
      "one more iteration. 16925\n"
     ]
    }
   ],
   "source": [
    "all_high_pos = approx_topk(tmp, th0=1, verbose=True) # torch.nonzero((tmp > th) & (tmp < th_max)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3207aed9-f99e-4109-9160-2d62f31e8b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_same = False\n",
    "reverse_list = False\n",
    "only_ascii = True\n",
    "only_alnum = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126924a",
   "metadata": {},
   "source": [
    "kodeks przypis [, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0fab412a-3f2e-4e0c-b11e-c462b17b6191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('].', '['),\n",
       " ('],', '['),\n",
       " (' ).', ' ('),\n",
       " (']', '['),\n",
       " ('...),', ' ('),\n",
       " ('przyp', ' ('),\n",
       " (' ),', ' ('),\n",
       " ('-).', ' ('),\n",
       " ('ZOBACZ', ' ['),\n",
       " ('zob', ' ('),\n",
       " ('przyp', ' ['),\n",
       " ('.]', '['),\n",
       " ('%)', ' ('),\n",
       " ('!),', ' ('),\n",
       " ('Ash', ' ('),\n",
       " ('].', ' ['),\n",
       " ('.].', '['),\n",
       " ('],', ' ['),\n",
       " (' rezygnacja', ' ('),\n",
       " ('!).', ' ('),\n",
       " ('prem', ' ('),\n",
       " ('\\x15', ' ('),\n",
       " ('...),', ' (\"'),\n",
       " ('orom', ' ('),\n",
       " ('%),', ' ('),\n",
       " ('ecie', ' ['),\n",
       " ('tul', '['),\n",
       " ('.].', ' ['),\n",
       " ('CZYTAJ', ' ['),\n",
       " ('etu', ' ['),\n",
       " ('-)', ' ('),\n",
       " ('przyp', ' (\"'),\n",
       " ('%).', ' ('),\n",
       " ('?),', ' ('),\n",
       " ('sic', ' ['),\n",
       " ('http', ' ('),\n",
       " (' ]', ' ['),\n",
       " ('Dzwonek', ' ('),\n",
       " (' ).', ' (\"'),\n",
       " ('!)', ' ('),\n",
       " ('demon', ' ('),\n",
       " (' realizacja', ' ('),\n",
       " ('dlaczego', ' ('),\n",
       " ('.]', ' ['),\n",
       " ('Dzwonek', ' (\"'),\n",
       " ('Kodeks', ' ['),\n",
       " ('Bir', ' ['),\n",
       " ('Ash', ' (\"'),\n",
       " ('eu', ' ('),\n",
       " ('przyp', ' (+')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_entries(tmp, all_high_pos, only_ascii=only_ascii, only_alnum=only_alnum, \n",
    "                exclude_same=exclude_same, tokens_list=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "39e840aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_vo(i1, i2, only_ascii=True, only_alnum=False, exclude_same=False, exclude_fuzzy=False, tokens_list=None):\n",
    "    W_V_tmp, W_O_tmp = W_V_heads[i1, i2, :], W_O_heads[i1, i2]\n",
    "    tmp = (emb_inv @ (W_V_tmp @ W_O_tmp) @ emb)\n",
    "    all_high_pos = approx_topk(tmp, th0=1, verbose=True) # torch.nonzero((tmp > th) & (tmp < th_max)).tolist()\n",
    "    return get_top_entries(tmp, all_high_pos, only_ascii=only_ascii, only_alnum=only_alnum, \n",
    "                           exclude_same=exclude_same, exclude_fuzzy=exclude_fuzzy, tokens_list=tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4422d4c",
   "metadata": {},
   "source": [
    "chcecie wasze nasze opisujące osby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a9412b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 1\n",
      "one more iteration. 15804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' chcecie', ' waszych'),\n",
       " (' chcecie', ' waszym'),\n",
       " (' musicie', ' waszych'),\n",
       " (' chcecie', ' wam'),\n",
       " (' waszych', ' waszych'),\n",
       " (' chcecie', ' chcecie'),\n",
       " (' musicie', ' waszym'),\n",
       " ('ujecie', ' waszych'),\n",
       " (' chcecie', ' waszego'),\n",
       " (' musicie', ' musicie'),\n",
       " (' musicie', ' wam'),\n",
       " (' chcecie', ' waszej'),\n",
       " (' chcecie', ' was'),\n",
       " (' chcecie', ' wasze'),\n",
       " (' chcecie', ' wami'),\n",
       " (' znacie', ' waszych'),\n",
       " ('ujecie', ' waszym'),\n",
       " (' musicie', ' waszego'),\n",
       " (' macie', ' waszych'),\n",
       " (' musicie', ' waszej'),\n",
       " (' musicie', ' wami'),\n",
       " (' wasze', ' waszych'),\n",
       " (' chcecie', 'ujecie'),\n",
       " (' musicie', ' was'),\n",
       " (' chcecie', 'czycie'),\n",
       " (' znacie', ' waszym'),\n",
       " ('ujecie', ' wam'),\n",
       " (' waszych', ' waszym'),\n",
       " (' musicie', ' wasze'),\n",
       " (' widzicie', ' waszych'),\n",
       " (' chcecie', 'dzicie'),\n",
       " (' waszych', ' wam'),\n",
       " (' chcecie', ' musicie'),\n",
       " ('ujecie', ' was'),\n",
       " (' chcecie', ' zobaczycie'),\n",
       " ('ujecie', ' waszego'),\n",
       " (' chcecie', ' wasz'),\n",
       " (' robicie', ' waszych'),\n",
       " (' waszym', ' waszych'),\n",
       " (' rozumiecie', ' waszych'),\n",
       " (' znacie', ' wam'),\n",
       " (' macie', ' wam'),\n",
       " (' znacie', ' waszego'),\n",
       " ('ujecie', ' wasze'),\n",
       " (' waszych', ' waszego'),\n",
       " (' macie', ' waszym'),\n",
       " ('ujecie', ' wami'),\n",
       " (' znacie', ' waszej'),\n",
       " (' widzicie', ' waszym'),\n",
       " (' waszych', ' waszej')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = 20, 4\n",
    "\n",
    "w_vo(i1, i2, only_ascii=True, only_alnum=False, exclude_same=False, exclude_fuzzy=False, tokens_list=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5567ab",
   "metadata": {},
   "source": [
    "prezydent, lewica, rakiety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9436d403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 75015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' jach', 'nomor'),\n",
       " (' samochodowy', ' kierowcy'),\n",
       " ('kowiak', ' wojewoda'),\n",
       " ('owiak', ' wojewoda'),\n",
       " ('owiak', ' wojewody'),\n",
       " (' Prezydent', 'ckiewicz'),\n",
       " ('Prezydent', 'ckiewicz'),\n",
       " (' samochodowy', ' pojazdu'),\n",
       " ('mady', 'hady'),\n",
       " (' jacht', 'nomor'),\n",
       " ('Trump', ' konstytucyjne'),\n",
       " (' lotnicze', ' piloci'),\n",
       " (' morska', 'hire'),\n",
       " (' chleb', ' zmywar'),\n",
       " (' rolne', ' gospodarstwach'),\n",
       " (' kontrah', ' wyceny'),\n",
       " ('loe', 'drow'),\n",
       " (' jach', ' Navy'),\n",
       " (' przeciwpancer', 'pancer'),\n",
       " ('stadt', ' Poniat'),\n",
       " ('Porucznik', ' Poniat'),\n",
       " (' auta', ' kierowcy'),\n",
       " (' rowerowe', ' kierowcy'),\n",
       " ('dun', 'hire'),\n",
       " ('woje', 'hady'),\n",
       " (' rakiet', 'pancer'),\n",
       " (' helikopter', ' lotnictwa'),\n",
       " (' rolne', ' gospodarstwa'),\n",
       " ('szenko', ' powiat'),\n",
       " (' helikopter', ' samolocie'),\n",
       " (' Renault', ' wagony'),\n",
       " (' rolne', ' Gospodarstwa'),\n",
       " (' rolnego', ' Gospodarstwa'),\n",
       " ('sfak', ' NFZ'),\n",
       " (' samochodowych', ' kierowcy'),\n",
       " (' floty', ' Navy'),\n",
       " (' bmw', ' kierowcy'),\n",
       " ('Tadeusz', ' Poniat'),\n",
       " ('rek', ' stypendium'),\n",
       " ('stok', ' Poniat'),\n",
       " ('lon', 'hady'),\n",
       " ('aux', ' Konstytucyjnym'),\n",
       " (' samochodowy', ' hamul'),\n",
       " (' Insta', ' miliar'),\n",
       " ('umar', 'hady'),\n",
       " (' Lec', ' dermato'),\n",
       " (' rolnych', ' Gospodarstwa'),\n",
       " ('kowiak', ' powiaty'),\n",
       " (' helikopter', 'nomor'),\n",
       " (' hydro', 'mura')]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = 10, 15\n",
    "\n",
    "w_vo(i1, i2, only_ascii=True, only_alnum=False, exclude_same=False, exclude_fuzzy=False, tokens_list=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c470b23",
   "metadata": {},
   "source": [
    "okresla polozenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "dc1e6202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 10\n",
      "one more iteration. 5182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' ranem', ' nad'),\n",
       " ('ornie', ' przez'),\n",
       " ('datek', ' nad'),\n",
       " ('arl', ' przed'),\n",
       " (' pewno', ' na'),\n",
       " ('spodziewanie', ' nad'),\n",
       " (' razu', ' od'),\n",
       " ('miernie', ' nad'),\n",
       " (' okazji', ' przy'),\n",
       " (' wsk', ' na'),\n",
       " (' czele', ' na'),\n",
       " ('orne', ' przez'),\n",
       " ('godziny', ' nad'),\n",
       " ('sione', ' przed'),\n",
       " (' ranem', 'nad'),\n",
       " ('natural', ' nad'),\n",
       " ('przewo', ' nad'),\n",
       " (' Duna', ' nad'),\n",
       " ('ktory', ' do'),\n",
       " ('miar', ' nad'),\n",
       " (' dobra', ' dla'),\n",
       " (' Jezi', ' nad'),\n",
       " ('spodzie', ' nad'),\n",
       " (' niedawna', ' do'),\n",
       " ('pisie', ' pod'),\n",
       " (' wygody', ' dla'),\n",
       " ('arcie', ' przed'),\n",
       " (' sumie', ' w'),\n",
       " ('miernie', 'nad'),\n",
       " ('tek', ' pod'),\n",
       " ('wcze', ' przed'),\n",
       " ('mier', ' nad'),\n",
       " ('hala', ' pod'),\n",
       " ('ornie', ' przeze'),\n",
       " (' uboczu', ' na'),\n",
       " (' podstawie', ' na'),\n",
       " (' ranem', 'Nad'),\n",
       " ('czesne', ' do'),\n",
       " ('granicznych', ' nad'),\n",
       " ('wiska', ' przez'),\n",
       " (' barkach', ' na'),\n",
       " ('ornie', 'przez'),\n",
       " (' odmiany', ' dla'),\n",
       " ('niego', ' przed'),\n",
       " (' plecami', ' za'),\n",
       " (' dobi', ' na'),\n",
       " ('przewodni', ' nad'),\n",
       " (' koniec', ' pod'),\n",
       " (' razie', ' na'),\n",
       " (' potrzeby', ' na')]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = 21, 7\n",
    "\n",
    "w_vo(i1, i2, only_ascii=True, only_alnum=False, exclude_same=False, exclude_fuzzy=False, tokens_list=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b4fd2ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 173\n",
      "one more iteration. 34601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' trucizny', ' truci'),\n",
       " (' nowotwor', ' onko'),\n",
       " (' trucizny', ' trucizny'),\n",
       " (' AIDS', ' AIDS'),\n",
       " (' astro', ' astro'),\n",
       " (' immuno', ' immuno'),\n",
       " (' trup', ' trup'),\n",
       " (' HIV', ' AIDS'),\n",
       " (' gazy', ' gazy'),\n",
       " (' radioaktyw', ' radioaktyw'),\n",
       " (' grypy', ' szczepion'),\n",
       " (' przeszcze', ' przeszcze'),\n",
       " (' wirusem', ' wirus'),\n",
       " (' wirusem', ' wirusa'),\n",
       " (' epidem', ' epidem'),\n",
       " (' truci', ' truci'),\n",
       " (' peni', ' peni'),\n",
       " (' trup', ' trupa'),\n",
       " (' Wetery', ' Wetery'),\n",
       " (' HIV', ' HIV'),\n",
       " (' szczepionki', ' szczepionki'),\n",
       " (' ofierze', ' ofierze'),\n",
       " (' prochu', ' prochu'),\n",
       " (' grypy', ' grypy'),\n",
       " (' czaszki', ' czaszki'),\n",
       " ('zdrowi', 'zdrowi'),\n",
       " (' szczepionki', ' szczepion'),\n",
       " (' uzdrowi', ' uzdrowi'),\n",
       " (' grypy', ' szczepionki'),\n",
       " (' grypy', ' wirus'),\n",
       " (' geode', ' geode'),\n",
       " (' nowotwor', ' nowotw'),\n",
       " (' ofierze', ' ofiara'),\n",
       " (' raka', ' onko'),\n",
       " (' onko', ' onko'),\n",
       " (' strzyka', ' strzyka'),\n",
       " (' trupa', ' trup'),\n",
       " (' wetery', ' wetery'),\n",
       " (' epidem', ' epidemio'),\n",
       " (' wirusem', ' wirusem'),\n",
       " (' grypy', ' epidemio'),\n",
       " (' AIDS', ' HIV'),\n",
       " (' odpadami', ' odpadami'),\n",
       " (' spal', ' spal'),\n",
       " (' szczepionki', ' szczepionka'),\n",
       " (' spale', ' spale'),\n",
       " (' higie', ' higie'),\n",
       " (' abor', ' abor'),\n",
       " (' Cmenta', ' Cmenta'),\n",
       " (' energo', ' energo')]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = 22, 9\n",
    "\n",
    "w_vo(i1, i2, only_ascii=True, only_alnum=True, exclude_same=False, exclude_fuzzy=False, tokens_list=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999ae658",
   "metadata": {},
   "source": [
    "imiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8b9db6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 5\n",
      "one more iteration. 68762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('gla', ' Doug'),\n",
       " ('sarzu', ' Jones'),\n",
       " ('gla', ' Dou'),\n",
       " ('losci', ' Danny'),\n",
       " ('shire', ' Tommy'),\n",
       " ('anom', ' Fabi'),\n",
       " ('tetu', ' Violet'),\n",
       " ('ach', ' Josh'),\n",
       " ('aryjnych', ' Romu'),\n",
       " ('dzu', ' Amanda'),\n",
       " ('orki', ' Davis'),\n",
       " ('sarzu', ' Jone'),\n",
       " ('dalem', ' Ryszar'),\n",
       " ('lip', ' Phil'),\n",
       " ('ranki', ' Piot'),\n",
       " ('nictw', ' Molly'),\n",
       " ('lotte', ' Charlotte'),\n",
       " ('oty', ' Cole'),\n",
       " ('nold', ' Rey'),\n",
       " ('fina', ' Leo'),\n",
       " ('osci', ' Mitch'),\n",
       " ('ic', ' Artem'),\n",
       " ('gla', ' Douglas'),\n",
       " ('ftu', ' Ellie'),\n",
       " ('ach', ' Gam'),\n",
       " ('derlan', ' Molly'),\n",
       " ('del', ' Bogdan'),\n",
       " ('nim', ' Anto'),\n",
       " ('alie', ' Nathan'),\n",
       " ('tyfika', ' Violet'),\n",
       " ('st', ' Cami'),\n",
       " ('rence', ' Violet'),\n",
       " ('rzu', ' Waw'),\n",
       " ('prawy', ' Szymon'),\n",
       " ('nowskim', ' Marty'),\n",
       " ('tyny', ' Alber'),\n",
       " ('osc', ' Mitch'),\n",
       " ('ngu', ' Lyn'),\n",
       " ('toli', ' Feliks'),\n",
       " ('stia', ' Seba'),\n",
       " ('aryjnych', ' Catherine'),\n",
       " ('lett', ' Scar'),\n",
       " ('isa', ' Lou'),\n",
       " ('tto', ' Lance'),\n",
       " ('tanu', ' Ryan'),\n",
       " ('lomet', ' Agata'),\n",
       " ('ria', ' Jeremy'),\n",
       " ('fus', ' Vale'),\n",
       " ('cen', ' Sara'),\n",
       " ('cyzm', ' Maury')]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = 20, 7\n",
    "\n",
    "w_vo(i1, i2, only_ascii=True, only_alnum=True, exclude_same=False, exclude_fuzzy=False, tokens_list=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49638ac2-455c-4441-aa67-4fde61c9ea83",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### $W_{QK}$ Interpretation\n",
    "\n",
    "Q - Query, K - Key co na co odpowiada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbfc882-3bf7-46e5-8e6a-f6a245ceb7bf",
   "metadata": {},
   "source": [
    "Choose **layer** and **head** here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c43e9c99-f9bf-4eae-b215-289a7f630ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 13)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i1, i2 = np.random.randint(num_layers), np.random.randint(num_heads)\n",
    "i1, i2 = 20, 13\n",
    "i1, i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e74cb0c2-c39f-42ce-a87c-91125207d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q_tmp, W_K_tmp = W_Q_heads[i1, i2, :], W_K_heads[i1, i2, :]\n",
    "tmp2 = (emb_inv @ (W_Q_tmp @ W_K_tmp.T) @ emb_inv.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "26a461fb-bcbc-477a-be76-20f5b52f329d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 755\n"
     ]
    }
   ],
   "source": [
    "all_high_pos = approx_topk(tmp2, th0=1, verbose=True) # torch.nonzero((tmp2 > th2) & (tmp2 < th_max2)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "16706487-23da-4e39-9799-641528d2e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_same = False\n",
    "reverse_list = False\n",
    "only_ascii = True\n",
    "only_alnum = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4e402a1a-8481-4481-9bad-a10f70542b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' waszych', ' wasze'),\n",
       " (' waszych', ' twoje'),\n",
       " (' waszych', ' twoi'),\n",
       " (' waszych', ' twoimi'),\n",
       " (' jakbym', 'mam'),\n",
       " (' waszym', ' wasze'),\n",
       " (' waszych', ' waszych'),\n",
       " (' waszych', ' twoich'),\n",
       " (' naszych', ' twoje'),\n",
       " (' waszego', ' wasze'),\n",
       " (' waszych', ' wasza'),\n",
       " (' waszych', ' twoim'),\n",
       " (' naszych', ' wasze'),\n",
       " (' Waszej', ' twoje'),\n",
       " (' waszych', ' wasz'),\n",
       " (' waszym', ' twoje'),\n",
       " (' wasze', ' wasze'),\n",
       " (' waszej', ' wasze'),\n",
       " (' waszych', ' twojej'),\n",
       " (' wasza', ' wasze'),\n",
       " (' waszego', ' twoje'),\n",
       " (' Waszej', ' wasze'),\n",
       " (' waszego', ' twoi'),\n",
       " (' naszymi', ' wasze'),\n",
       " (' wasz', ' wasze'),\n",
       " (' wasz', ' waszych'),\n",
       " (' naszych', ' twoich'),\n",
       " (' naszych', ' twoimi'),\n",
       " (' waszych', ' waszym'),\n",
       " (' waszym', ' twoi'),\n",
       " (' waszego', ' wasza'),\n",
       " (' waszej', ' twoje'),\n",
       " (' naszych', ' Twoje'),\n",
       " (' waszych', ' twoja'),\n",
       " (' wasza', ' twoje'),\n",
       " (' waszym', ' waszych'),\n",
       " (' naszymi', ' twoje'),\n",
       " (' wasze', ' twoje'),\n",
       " (' waszych', ' Twoje'),\n",
       " (' Waszej', ' twojej'),\n",
       " (' wasz', ' twoi'),\n",
       " (' waszej', ' twoi'),\n",
       " (' naszych', ' waszych'),\n",
       " (' naszych', ' twoim'),\n",
       " (' wasza', ' wasza'),\n",
       " (' waszego', ' waszych'),\n",
       " (' waszych', ' Wasza'),\n",
       " (' wasze', ' waszych'),\n",
       " (' waszym', ' twoimi'),\n",
       " (' twoich', ' wasze')]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_entries(tmp2, all_high_pos, only_ascii=only_ascii, only_alnum=only_alnum, exclude_same=exclude_same, \n",
    "                tokens_list=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9eab6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_qk(i1, i2, only_ascii=True, only_alnum=False, exclude_same=False, exclude_fuzzy=False, tokens_list=None):\n",
    "    W_Q_tmp, W_K_tmp = W_Q_heads[i1, i2, :], W_K_heads[i1, i2, :]\n",
    "    tmp2 = (emb_inv @ (W_Q_tmp @ W_K_tmp.T) @ emb_inv.T)\n",
    "    all_high_pos = approx_topk(tmp2, th0=1, verbose=True) # torch.nonzero((tmp2 > th2) & (tmp2 < th_max2)).tolist()\n",
    "    return get_top_entries(tmp2, all_high_pos, only_ascii=only_ascii, only_alnum=only_alnum, \n",
    "                           exclude_same=exclude_same, exclude_fuzzy=exclude_fuzzy, tokens_list=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d40e0a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 11\n",
      "one more iteration. 399\n",
      "one more iteration. 43253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('\").', '\").'),\n",
       " ('\").', '\"),'),\n",
       " ('\").', '\")'),\n",
       " ('\"),', '\").'),\n",
       " ('\")', '\").'),\n",
       " ('\")', '\")'),\n",
       " ('\"),', '\")'),\n",
       " ('\")', '\"),'),\n",
       " ('\"),', '\"),'),\n",
       " ('\").', ' ).'),\n",
       " ('\").', '.).'),\n",
       " ('\").', ');'),\n",
       " (')?', '\")'),\n",
       " (')?', '\").'),\n",
       " (')?', '\"),'),\n",
       " ('\").', ').'),\n",
       " ('\").', ')?'),\n",
       " (');', '\").'),\n",
       " ('\").', '.)'),\n",
       " ('\").', '%).'),\n",
       " ('\").', '?).'),\n",
       " ('\")', ');'),\n",
       " ('\"),', ')?'),\n",
       " ('\")', ')?'),\n",
       " ('\").', ' ),'),\n",
       " ('\"),', ');'),\n",
       " ('?).', '\").'),\n",
       " (');', '\"),'),\n",
       " ('\").', '!).'),\n",
       " ('!).', '\").'),\n",
       " ('\").', '.),'),\n",
       " ('?)', '\")'),\n",
       " ('):', '\")'),\n",
       " ('\")', '.)'),\n",
       " ('\").', '...).'),\n",
       " ('?).', '\"),'),\n",
       " ('\").', '%)'),\n",
       " ('):', '\").'),\n",
       " ('?)', '\"),'),\n",
       " ('):', '\"),'),\n",
       " ('?).', '\")'),\n",
       " ('\"),', ' ).'),\n",
       " ('\").', '?)'),\n",
       " ('\")', '?)'),\n",
       " ('\").', '?),'),\n",
       " ('\").', '),'),\n",
       " ('\").', '):'),\n",
       " ('\"),', '.)'),\n",
       " ('?)', '\").'),\n",
       " ('\"),', ' ),')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = 9, 5\n",
    "\n",
    "w_qk(i1, i2, only_ascii=True, only_alnum=False, exclude_same=False, exclude_fuzzy=False, tokens_list=tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766be4c",
   "metadata": {},
   "source": [
    "organizacje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c124a7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 13437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' funduszu', ' Schengen'),\n",
       " (' fundusz', ' UEFA'),\n",
       " (' klucza', 'Samolot'),\n",
       " (' klucza', ' lotniczej'),\n",
       " (' fundusz', ' Schengen'),\n",
       " (' jednostki', ' Stoczni'),\n",
       " (' klucza', ' lotnicze'),\n",
       " (' funduszu', ' EURO'),\n",
       " (' Sojuszu', ' NATO'),\n",
       " (' fundusz', ' KRUS'),\n",
       " (' jednostki', ' stoczni'),\n",
       " (' nos', ' lotniczy'),\n",
       " (' fundusz', ' EURO'),\n",
       " (' sieci', ' ryb'),\n",
       " (' fundusz', ' OFE'),\n",
       " (' wody', ' podwodnych'),\n",
       " (' klucza', ' lotniczy'),\n",
       " (' sieci', ' ryba'),\n",
       " (' Sojusz', ' NATO'),\n",
       " (' funduszu', ' UEFA'),\n",
       " (' DJ', ' dywizji'),\n",
       " (' klucza', ' lotniczych'),\n",
       " (' klucza', ' Lotnictwa'),\n",
       " (' klucza', ' lotniczego'),\n",
       " (' wody', ' ryba'),\n",
       " (' Luf', ' 1943'),\n",
       " (' konty', ' NATO'),\n",
       " (' fundusz', ' UE'),\n",
       " (' organizacji', ' Schengen'),\n",
       " (' wody', ' zatoki'),\n",
       " (' Fron', ' ZSRR'),\n",
       " (' MW', ' Marynar'),\n",
       " (' nos', ' lotniczego'),\n",
       " (' jednostek', ' Stoczni'),\n",
       " (' wody', ' ryby'),\n",
       " (' klucza', ' samolot'),\n",
       " (' mostek', ' kaju'),\n",
       " (' funduszu', ' UE'),\n",
       " (' zrzu', ' 1944'),\n",
       " (' nos', ' samolocie'),\n",
       " (' klucza', ' lotni'),\n",
       " (' mandat', ' Palestyny'),\n",
       " (' Luf', ' 1942'),\n",
       " (' MW', ' Marynarki'),\n",
       " (' zrzu', ' 1943'),\n",
       " (' bander', ' morski'),\n",
       " (' klucza', ' lot'),\n",
       " (' wody', ' ryb'),\n",
       " (' klucza', ' samolocie'),\n",
       " (' nos', ' samolot')]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = 0, 7\n",
    "\n",
    "w_qk(i1, i2, only_ascii=True, only_alnum=False, exclude_same=False, exclude_fuzzy=False, tokens_list=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415115f3",
   "metadata": {},
   "source": [
    "Pis, sejm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "70ac9354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 149\n",
      "one more iteration. 162255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('leen', 'isen'),\n",
       " ('gebjorg', 'owiska'),\n",
       " ('encji', 'zitel'),\n",
       " ('mera', 'ssen'),\n",
       " (' MINIST', ' MINIST'),\n",
       " ('gis', 'mowski'),\n",
       " ('gebjorg', 'isen'),\n",
       " ('anna', 'jewskiego'),\n",
       " ('encji', 'leran'),\n",
       " (' PIS', 'ktywi'),\n",
       " (' Departam', ' MINIST'),\n",
       " (' terapeu', 'kompen'),\n",
       " ('uszko', 'szowi'),\n",
       " ('encji', 'ktywi'),\n",
       " ('encji', 'bin'),\n",
       " ('lotu', 'szewskiego'),\n",
       " (' polityczny', ' MINIST'),\n",
       " (' PiSu', 'ktywi'),\n",
       " (' Koali', 'niesiono'),\n",
       " ('encji', 'nia'),\n",
       " (' rozowepaski', 'czniku'),\n",
       " (' akademi', ' szczy'),\n",
       " (' Narodowa', ' MINIST'),\n",
       " ('COP', 'niesiono'),\n",
       " ('gebjorg', 'randa'),\n",
       " (' rozowepaski', 'lomet'),\n",
       " ('encji', 'oll'),\n",
       " (' KRRiT', 'ktywi'),\n",
       " ('lecz', 'dar'),\n",
       " ('lette', 'ssen'),\n",
       " ('gebjorg', 'jew'),\n",
       " ('encji', 'pnie'),\n",
       " ('lette', 'szewski'),\n",
       " ('encji', 'gini'),\n",
       " ('gebjorg', 'nowu'),\n",
       " ('anna', 'szewskiego'),\n",
       " ('lamin', 'ssen'),\n",
       " (' lgbt', 'rzmi'),\n",
       " ('lette', 'rance'),\n",
       " ('gy', 'niesione'),\n",
       " ('lecz', 'obra'),\n",
       " ('ggy', 'dnicki'),\n",
       " ('gebjorg', 'niew'),\n",
       " (' rozowepaski', 'dne'),\n",
       " ('jeb', 'dar'),\n",
       " ('leen', 'hn'),\n",
       " ('znesu', 'rozu'),\n",
       " ('eksu', 'lomet'),\n",
       " (' rozowepaski', 'cznika'),\n",
       " ('alka', 'nikowa')]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = 7, 9\n",
    "\n",
    "w_qk(i1, i2, only_ascii=True, only_alnum=True, exclude_same=False, exclude_fuzzy=False, tokens_list=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86117a60",
   "metadata": {},
   "source": [
    "podroze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c1e2e490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 1\n",
      "one more iteration. 2021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' pieszo', ' pieszo'),\n",
       " (' jechali', ' pieszo'),\n",
       " (' dojazd', ' pieszo'),\n",
       " (' spaceru', ' pieszo'),\n",
       " (' jech', ' pieszo'),\n",
       " (' wsiada', ' pieszo'),\n",
       " (' przemieszczania', ' pieszo'),\n",
       " (' space', ' pieszo'),\n",
       " (' maszer', ' pieszo'),\n",
       " (' pieszych', ' pieszo'),\n",
       " (' przemieszcza', ' pieszo'),\n",
       " (' jedzie', ' pieszo'),\n",
       " (' przyjechali', ' pieszo'),\n",
       " (' spacer', ' pieszo'),\n",
       " (' trasy', ' pieszo'),\n",
       " (' spacery', ' pieszo'),\n",
       " (' pieszego', ' pieszo'),\n",
       " (' rozpak', ' rozpak'),\n",
       " (' trasa', ' pieszo'),\n",
       " (' przecha', ' pieszo'),\n",
       " (' stacjon', ' stacjon'),\n",
       " (' przyjedzie', ' pieszo'),\n",
       " (' pieszy', ' pieszo'),\n",
       " (' trasie', ' pieszo'),\n",
       " (' wysiada', ' pieszo'),\n",
       " (' korytarz', ' schodami'),\n",
       " (' przewie', ' pieszo'),\n",
       " (' pieszo', ' rowerem'),\n",
       " (' wyprawa', ' pieszo'),\n",
       " (' drzwi', ' drzwi'),\n",
       " (' przetransport', ' pieszo'),\n",
       " ('jechali', ' pieszo'),\n",
       " (' rowerem', ' pieszo'),\n",
       " (' autokar', ' pieszo'),\n",
       " (' konno', ' pieszo'),\n",
       " (' wioz', ' pieszo'),\n",
       " ('jazdu', ' pieszo'),\n",
       " (' pulpi', ' pulpi'),\n",
       " ('kilomet', ' pieszo'),\n",
       " (' przejazdu', ' pieszo'),\n",
       " (' wycieczka', ' pieszo'),\n",
       " (' wyru', ' pieszo'),\n",
       " (' pojechali', ' pieszo'),\n",
       " (' wyemi', ' dobrowolnie'),\n",
       " (' ewakuacji', ' pieszo'),\n",
       " (' wyprawy', ' pieszo'),\n",
       " (' przyjazd', ' pieszo'),\n",
       " (' wyjazd', ' pieszo'),\n",
       " (' wyjechali', ' pieszo'),\n",
       " ('maszer', ' pieszo')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = 22, 5\n",
    "\n",
    "w_qk(i1, i2, only_ascii=True, only_alnum=True, exclude_same=False, exclude_fuzzy=False, tokens_list=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b1c83",
   "metadata": {},
   "source": [
    "przeklenstwa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bf995b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 179\n",
      "one more iteration. 4992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' T', 'day'),\n",
       " (' N', 'jscy'),\n",
       " (' chuj', ' chuj'),\n",
       " (' etc', ' etc'),\n",
       " (' itp', ' etc'),\n",
       " (' H', 'alla'),\n",
       " ('pierdo', ' jeb'),\n",
       " (' H', 'HA'),\n",
       " (' R', 'rene'),\n",
       " (' M', 'taria'),\n",
       " (' R', 'sung'),\n",
       " (' N', 'iers'),\n",
       " (' itp', ' itp'),\n",
       " (' N', 'Obiekt'),\n",
       " (' M', 'wychw'),\n",
       " (' N', ' ktorzy'),\n",
       " (' p', 'jpg'),\n",
       " (' chuj', ' jeb'),\n",
       " (' P', 'znesu'),\n",
       " (' etc', ' itp'),\n",
       " (' M', 'sztof'),\n",
       " (' L', ' Gospodarczej'),\n",
       " (' F', 'Pilot'),\n",
       " (' N', ' przyklad'),\n",
       " (' G', 'Gri'),\n",
       " (' chuja', ' chuj'),\n",
       " (' M', 'stora'),\n",
       " (' kurwa', ' jeb'),\n",
       " (' itd', ' etc'),\n",
       " (' T', 'ciarze'),\n",
       " (' J', ' OPowi'),\n",
       " (' l', 'Dzwonek'),\n",
       " (' M', 'chium'),\n",
       " (' P', 'omosci'),\n",
       " (' kurwa', ' pierdol'),\n",
       " (' R', ' Szkolnictwa'),\n",
       " (' R', 'VID'),\n",
       " ('pierdol', ' jeb'),\n",
       " (' chuja', ' jeb'),\n",
       " ('pierdala', ' chuj'),\n",
       " (' J', 'zawodni'),\n",
       " (' M', 'cjusza'),\n",
       " (' kurwa', ' chuj'),\n",
       " (' jeb', ' jeb'),\n",
       " (' R', 'stur'),\n",
       " ('jeb', ' chuj'),\n",
       " (' D', 'Kal'),\n",
       " (' F', 'hem'),\n",
       " (' N', 'cznosci'),\n",
       " (' J', 'ROW')]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = 23, 9\n",
    "\n",
    "w_qk(i1, i2, only_ascii=True, only_alnum=True, exclude_same=False, exclude_fuzzy=False, tokens_list=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d54477",
   "metadata": {},
   "source": [
    "ilosci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "37e5086f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 334\n",
      "one more iteration. 353457\n",
      "one more iteration. 3623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' ani', ' ani'),\n",
       " (' ani', ' zadnej'),\n",
       " (' ani', ' zadnego'),\n",
       " (' zadnego', ' ani'),\n",
       " (' komukolwiek', ' komukolwiek'),\n",
       " (' zadnej', ' ani'),\n",
       " (' dowolny', ' dowolnie'),\n",
       " (' dowolnie', ' dowolnie'),\n",
       " (' ani', ' zadnych'),\n",
       " (' dowolne', ' dowolnie'),\n",
       " (' dowolnej', ' dowolnie'),\n",
       " (' komukolwiek', ' kogokolwiek'),\n",
       " (' dowolnym', ' dowolnie'),\n",
       " (' zadnych', ' ani'),\n",
       " (' zadnego', ' zadnego'),\n",
       " (' kogokolwiek', ' kogokolwiek'),\n",
       " (' kogokolwiek', ' komukolwiek'),\n",
       " (' jakiejkolwiek', ' komukolwiek'),\n",
       " (' jakimkolwiek', ' komukolwiek'),\n",
       " (' zadnego', ' zadnej'),\n",
       " (' dowolnego', ' dowolnie'),\n",
       " (' dowolnej', ' dowolnej'),\n",
       " (' jakiegokolwiek', ' komukolwiek'),\n",
       " (' ani', ' nigdzie'),\n",
       " (' drugiemu', ' drugiemu'),\n",
       " (' kiedykolwiek', ' kiedykolwiek'),\n",
       " (' czegokolwiek', ' czegokolwiek'),\n",
       " (' jakimkolwiek', ' kogokolwiek'),\n",
       " (' czegokolwiek', ' komukolwiek'),\n",
       " (' drugie', ' druga'),\n",
       " (' drugiemu', ' obu'),\n",
       " (' czegokolwiek', ' kogokolwiek'),\n",
       " (' jakichkolwiek', ' ani'),\n",
       " (' dowolne', ' dowolne'),\n",
       " (' drugie', ' drugie'),\n",
       " ('cker', 'przyp'),\n",
       " ('dl', 'ons'),\n",
       " (' dziennie', ' dziennie'),\n",
       " (' zadnego', ' zadnych'),\n",
       " (' zadnej', ' zadnego'),\n",
       " (' ani', ' komukolwiek'),\n",
       " (' jakiejkolwiek', ' kogokolwiek'),\n",
       " (' dowolnej', ' dowolne'),\n",
       " (' dowolne', ' dowolnej'),\n",
       " (' jakiegokolwiek', ' kogokolwiek'),\n",
       " (' drugich', ' drugich'),\n",
       " (' zadnego', ' komukolwiek'),\n",
       " (' zadnej', ' zadnej'),\n",
       " (' drudzy', ' drugi'),\n",
       " (' drugie', ' drugi')]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = 19, 1\n",
    "\n",
    "w_qk(i1, i2, only_ascii=True, only_alnum=True, exclude_same=False, exclude_fuzzy=False, tokens_list=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465544c3",
   "metadata": {},
   "source": [
    "zwroty osobowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "69583f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 4606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' Waszej', ' pan'),\n",
       " (' Twoim', ' Pana'),\n",
       " (' twoim', ' pana'),\n",
       " (' twoim', ' Pana'),\n",
       " (' twoim', ' Panu'),\n",
       " (' twoim', ' pan'),\n",
       " (' Twoim', ' pana'),\n",
       " (' twoim', ' panie'),\n",
       " (' Twoim', ' pan'),\n",
       " (' Twoim', ' panie'),\n",
       " (' Waszej', ' Wasza'),\n",
       " (' Twoim', ' Pan'),\n",
       " (' Waszej', ' Pan'),\n",
       " (' waszym', ' pan'),\n",
       " (' Waszej', ' szanow'),\n",
       " (' tobie', ' pan'),\n",
       " (' Twoim', ' szanow'),\n",
       " (' Twoim', ' wasz'),\n",
       " (' Waszej', ' pana'),\n",
       " (' waszym', ' pana'),\n",
       " (' ciebie', ' Panu'),\n",
       " (' wam', ' pan'),\n",
       " (' Ciebie', ' Pana'),\n",
       " (' Waszej', ' Pana'),\n",
       " (' tobie', ' Panu'),\n",
       " (' Twoim', ' Panu'),\n",
       " (' tobie', ' pana'),\n",
       " (' Waszej', ' pani'),\n",
       " (' waszym', ' Pana'),\n",
       " (' twoim', ' panienka'),\n",
       " (' ciebie', ' pan'),\n",
       " (' Ciebie', ' Pan'),\n",
       " (' Wam', ' pan'),\n",
       " (' Twojej', ' Pana'),\n",
       " (' wasz', ' pan'),\n",
       " (' Twoim', ' Wasza'),\n",
       " (' twoimi', ' pana'),\n",
       " (' twoimi', ' pan'),\n",
       " (' twoim', ' panu'),\n",
       " (' twoim', ' Pan'),\n",
       " (' Wam', ' Pan'),\n",
       " (' waszym', ' wasz'),\n",
       " (' ciebie', ' Pana'),\n",
       " (' tobie', ' Pana'),\n",
       " (' waszym', ' Panu'),\n",
       " (' waszym', ' Wasza'),\n",
       " (' tobie', ' panie'),\n",
       " (' twojej', ' pan'),\n",
       " (' twojej', ' pana'),\n",
       " (' twoim', ' wasz')]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = 18, 5\n",
    "\n",
    "w_qk(i1, i2, only_ascii=True, only_alnum=True, exclude_same=False, exclude_fuzzy=False, tokens_list=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eaa26a",
   "metadata": {},
   "source": [
    "sporty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "18f43f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one more iteration. 0\n",
      "one more iteration. 0\n",
      "one more iteration. 1063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' wnim', ' oczym'),\n",
       " (' wtym', ' oczym'),\n",
       " (' turniej', ' turnieju'),\n",
       " (' turnieju', ' turnieju'),\n",
       " (' futbol', ' futbol'),\n",
       " (' znim', ' oczym'),\n",
       " (' tenis', ' futbol'),\n",
       " (' o', 'zwykopem'),\n",
       " (' film', ' filmowej'),\n",
       " (' ztego', ' oczym'),\n",
       " (' kolar', ' futbol'),\n",
       " (' lekkoatle', ' zawodnicy'),\n",
       " (' wnim', ' apotem'),\n",
       " (' wtym', ' Jego'),\n",
       " (' wdomu', ' ale'),\n",
       " (' kolar', ' zawodnicy'),\n",
       " (' filmowej', ' filmowej'),\n",
       " (' aktor', ' aktor'),\n",
       " (' turniej', ' turniej'),\n",
       " (' sport', ' zawodnicy'),\n",
       " (' film', ' filmowe'),\n",
       " (' wmo', ' oczym'),\n",
       " (' wtym', ' apotem'),\n",
       " (' zawodni', ' zawodnicy'),\n",
       " (' kibi', ' zawodnicy'),\n",
       " (' o', 'noscia'),\n",
       " (' wtej', ' oczym'),\n",
       " (' znim', ' apotem'),\n",
       " (' wna', ' oczym'),\n",
       " (' PZPN', ' turnieju'),\n",
       " (' wjego', ' Jego'),\n",
       " (' wdomu', ' oczym'),\n",
       " (' tenis', ' mecz'),\n",
       " (' tenis', ' zawodnicy'),\n",
       " (' kolar', ' sprin'),\n",
       " (' wnim', ' czego'),\n",
       " (' film', ' serialu'),\n",
       " (' muzycznym', ' muzycznej'),\n",
       " (' kolar', ' Tour'),\n",
       " (' film', ' filmowy'),\n",
       " (' muzyki', ' muzycznej'),\n",
       " (' sport', ' turnieju'),\n",
       " (' wtym', ' czego'),\n",
       " (' film', ' film'),\n",
       " (' teat', ' teat'),\n",
       " (' sportowych', ' zawodnicy'),\n",
       " (' lekkoatle', ' lekkoat'),\n",
       " (' filmowej', ' film'),\n",
       " (' lekkoatle', ' futbol'),\n",
       " (' kibi', ' kibice')]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1, i2 = 21, 12\n",
    "\n",
    "w_qk(i1, i2, only_ascii=True, only_alnum=True, exclude_same=False, exclude_fuzzy=False, tokens_list=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7dccf2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d3c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "golem-ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
